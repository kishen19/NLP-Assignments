{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Libraries\"\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, SimpleRNN, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import ngrams\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fetching data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_unwanted(text):\n",
    "    p = re.compile(\"(\\n|\\xa0|\\r|\\t|\\s|\\\\|\\\"|,|;|:|\\.|\\?|\\!|_|\\)|\\(|-|\\[|\\])+\")\n",
    "    w = p.sub(' ', text)\n",
    "    p = re.compile(\"speech [0-9]\")\n",
    "    return p.sub('', w)\n",
    "\n",
    "def clean_trump(link):\n",
    "    r = requests.get(link)\n",
    "    data = r.text\n",
    "    with open(\"trump_speeches.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(data)\n",
    "    data = data.lower()\n",
    "    sent_tokenized = sent_tokenize(data)\n",
    "    sentences = []\n",
    "    for i in range(len(sent_tokenized)):\n",
    "        sent = (\"<s> \" + strip_unwanted(sent_tokenized[i]).strip() + \" </s>\").encode('ascii', 'ignore').decode()\n",
    "        sentences.append(sent)\n",
    "    return sentences\n",
    "\n",
    "link = \"https://raw.githubusercontent.com/ryanmcdermott/trump-speeches/master/speeches.txt\"\n",
    "sentences = clean_trump(link)\n",
    "data_train, data_test = train_test_split(sentences, train_size=0.8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 5695\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Using tf.keras tokenizer I will assign indices to each type in the training data. Also, I will create two utility dictionaries\n",
    "which will held the mappings of word -> index and index->word\n",
    "\"\"\"\n",
    "tokenizer = Tokenizer(num_words=None, filters=[], lower=True, split=\" \") # data is already filtered, so no need to filter any thing else\n",
    "tokenizer.fit_on_texts(data_train) # fitting indices to words\n",
    "word_index = tokenizer.word_index # word -> index\n",
    "index_word = {} # index -> word\n",
    "for word in word_index:\n",
    "    index_word[word_index[word]] = word\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) # Vocabulary size\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "data_train = tokenizer.texts_to_sequences(data_train) # maps all words in the text to their respective indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ngram Language model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ngram model with add-k smoothing\n",
    "class ngram_model:\n",
    "    def __init__(self, ngram_size, data, tokenizer, word_index, index_word):\n",
    "        self.n = ngram_size\n",
    "        self.sentences = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.wordtoindex = word_index\n",
    "        self.indextoword = index_word\n",
    "        self.vocab = len(self.wordtoindex)\n",
    "        self.tokens = 0\n",
    "    \n",
    "    def train(self):\n",
    "        self.calculate_count()\n",
    "        self.calculate_mle()\n",
    "    \n",
    "    def evaluate(self, data):\n",
    "        data1 = self.tokenizer.texts_to_sequences(data)\n",
    "        min_prob= min(list(self.mle_ngrams.values()))\n",
    "        ans = 0\n",
    "        N = 1\n",
    "        probs = []\n",
    "        for sent in data1:\n",
    "            sent1 = []\n",
    "            for w in sent:\n",
    "                if w in self.wordtoindex:\n",
    "                    sent1.append(self.wordtoindex[w])\n",
    "                else:\n",
    "                    sent1.append(100000)\n",
    "            ngram = ngrams(sent1, self.n)\n",
    "            for gr in ngram:\n",
    "                N+=1\n",
    "                if gr in self.mle_ngrams:\n",
    "                    probs.append(self.mle_ngrams[gr])\n",
    "                else:\n",
    "                    probs.append(min_prob)\n",
    "        probs = np.asarray(probs)\n",
    "        ans = np.prod(np.power(probs, 1/N))\n",
    "        return 1/ans\n",
    "        \n",
    "    def calculate_count(self):\n",
    "        self.count_ngrams = defaultdict(int)\n",
    "        self.count_n_1_grams = defaultdict(int)\n",
    "        for sent in self.sentences:\n",
    "            ngram = ngrams(sent, self.n)\n",
    "            for gr in ngram:\n",
    "                self.count_ngrams[gr]+=1\n",
    "                if self.n==1:\n",
    "                    self.tokens+=1\n",
    "            if self.n!=1:\n",
    "                n_1_gram = ngrams(sent, self.n-1)\n",
    "                for gr in n_1_gram:\n",
    "                    self.count_n_1_grams[gr]+=1\n",
    "        \n",
    "    def calculate_mle(self):\n",
    "        self.mle_ngrams = defaultdict(int)\n",
    "        for ngram in self.count_ngrams:\n",
    "            self.mle_ngrams[ngram] = (self.count_ngrams[ngram])/( (self.tokens) if self.n==1 else (self.count_n_1_grams[ngram[:-1]]) )\n",
    "    \n",
    "    def generate_text(self, nsent):\n",
    "        flag = 1\n",
    "        prev = ('<s>',)\n",
    "        started = 0\n",
    "        ans = \"\"\n",
    "        while nsent>0:\n",
    "            selected = []\n",
    "            if flag and self.n!=1:\n",
    "                flag=0\n",
    "                prev = ('<s>',)\n",
    "                for ngram in self.mle_ngrams:\n",
    "                    if ngram[0]==\"<s>\":\n",
    "                        selected.append((ngram, self.mle_ngrams[ngram]))\n",
    "            if flag==0 or self.n==1:\n",
    "                for ngram in self.mle_ngrams:\n",
    "                    if ngram[:len(prev)-1]==prev[1:]:\n",
    "                        if ngram!=(self.wordtoindex[\"<s>\"],):\n",
    "                            selected.append((ngram, self.mle_ngrams[ngram]))\n",
    "                            \n",
    "            prob = [i[1] for i in selected]\n",
    "            a=sum(prob)\n",
    "            prob = [i/a for i in prob]\n",
    "            out = list(np.random.multinomial(10,prob))\n",
    "            i = out.index(max(out))\n",
    "            prev = selected[i][0]\n",
    "            if started:\n",
    "                if self.indextoword[prev[-1]]==\"</s>\":\n",
    "                    ans+= \". \"\n",
    "                else:\n",
    "                    ans+=' '+self.indextoword[prev[-1]]\n",
    "            else:\n",
    "                started = 1\n",
    "                w = list(prev)\n",
    "                w = [self.indextoword[i] for i in w]\n",
    "                if self.n==1:\n",
    "                    w[0] = w[0].capitalize()\n",
    "                    s = \" \".join(w[:-1])\n",
    "                else:\n",
    "                    w[1] = w[1].capitalize()\n",
    "                    s = \" \".join(w[1:-1])\n",
    "                ans+= s\n",
    "                if w[-1]==\"</s>\":\n",
    "                    ans+= \". \"\n",
    "                else:\n",
    "                    ans+=\" \"+w[-1]\n",
    "                    \n",
    "            if self.indextoword[prev[-1]]==\"</s>\":\n",
    "                started=0\n",
    "                flag=1\n",
    "                nsent-=1\n",
    "        return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating models for unigram, bigram, trigram and quadgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculating Unigrams, bigrams, trigrams and quadgrams\n",
    "unigram_model = ngram_model(ngram_size=1, data=data_train, tokenizer=tokenizer, index_word = index_word, word_index=word_index)\n",
    "bigram_model = ngram_model(ngram_size=2, data=data_train, tokenizer=tokenizer, index_word = index_word, word_index=word_index)\n",
    "trigram_model = ngram_model(ngram_size=3, data=data_train, tokenizer=tokenizer, index_word = index_word, word_index=word_index)\n",
    "quadgram_model = ngram_model(ngram_size=4, data=data_train, tokenizer=tokenizer, index_word = index_word,word_index=word_index)\n",
    "unigram_model.train()\n",
    "bigram_model.train()\n",
    "trigram_model.train()\n",
    "quadgram_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing unigrams: 5695 Possible unigrams: 5695\n",
      "Existing bigrams: 43793 Possible bigrams: 32433025\n",
      "Existing trigrams: 83719 Possible trigrams: 184706077375\n",
      "Existing quadgrams: 98446 Possible quadgrams: 1051901110650625\n"
     ]
    }
   ],
   "source": [
    "print(\"Existing unigrams:\", len(unigram_model.count_ngrams), \"Possible unigrams:\", unigram_model.vocab)\n",
    "print(\"Existing bigrams:\", len(bigram_model.count_ngrams), \"Possible bigrams:\", bigram_model.vocab**2)\n",
    "print(\"Existing trigrams:\", len(trigram_model.count_ngrams), \"Possible trigrams:\", trigram_model.vocab**3)\n",
    "print(\"Existing quadgrams:\", len(quadgram_model.count_ngrams), \"Possible quadgrams:\", quadgram_model.vocab**4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram model\n",
      "\n",
      ".  A the. . .  I they. \n",
      "\n",
      "\n",
      "Bigram model\n",
      "\n",
      " The border.  For the country.  Of the oil.  Rate.  Myself. \n",
      "\n",
      "\n",
      "Trigram model\n",
      "\n",
      "Bent or the yale but they say the words radical islam is coming in. Do anything. Not been a very nice as far as im concerned. Our country is going to be a fact we need to do that. Waste get rid of it. \n",
      "\n",
      "\n",
      "Quadgram model\n",
      "\n",
      "Example trump saudi arabia they make $1 billion a day. The president get away with it again. If we have a lot of people dont even know what the hell is going on. The world hates us. Year ago or so and he gave me a plaque because i supported him. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Unigram model\\n\")\n",
    "print(unigram_model.generate_text(5))\n",
    "print(\"\\n\\nBigram model\\n\")\n",
    "print(bigram_model.generate_text(5))\n",
    "print(\"\\n\\nTrigram model\\n\")\n",
    "print(trigram_model.generate_text(5))\n",
    "print(\"\\n\\nQuadgram model\\n\")\n",
    "print(quadgram_model.generate_text(5))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we increase the ngram number the readability of the text increases. This is because of more context. Also, the readability of the quadgram model is actually very nice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram model\n",
      "\n",
      "160663.81660976555\n",
      "\n",
      "\n",
      "Bigram model\n",
      "\n",
      "13116.456552524489\n",
      "\n",
      "\n",
      "Trigram model\n",
      "\n",
      "1512.6519174092455\n",
      "\n",
      "\n",
      "Quadgram model\n",
      "\n",
      "503.89016718551636\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Unigram model\\n\")\n",
    "print(unigram_model.evaluate(data_test))\n",
    "print(\"\\n\\nBigram model\\n\")\n",
    "print(bigram_model.evaluate(data_test))\n",
    "print(\"\\n\\nTrigram model\\n\")\n",
    "print(trigram_model.evaluate(data_test))\n",
    "print(\"\\n\\nQuadgram model\\n\")\n",
    "print(quadgram_model.evaluate(data_test))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class neural_model:\n",
    "    def __init__(self, data, tokenizer, word_index, index_word, max_sequence_len=15, hiddenlayer=\"Vanilla RNN\", epochs=20, batch_size=256):\n",
    "        self.data= data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sentences = data\n",
    "        self.wordtoindex = word_index\n",
    "        self.indextoword = index_word\n",
    "        self.max_sequence_len = max_sequence_len\n",
    "        self.vocab = len(self.wordtoindex) + 1\n",
    "        self.hidden = hiddenlayer\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def train(self):\n",
    "        self.prepare_data()\n",
    "        self.model = self.model_create()\n",
    "        \n",
    "    def evaluate(self, data):\n",
    "        data1 = self.tokenizer.texts_to_sequences(data)\n",
    "        input_sequences = []\n",
    "        for line in data1:\n",
    "            for i in range(1, len(line)):\n",
    "                n_gram_sequence = line[:i+1]\n",
    "                input_sequences.append(n_gram_sequence)\n",
    "        input_sequences = np.array(pad_sequences(input_sequences,maxlen = self.max_sequence_len, padding='pre'))\n",
    "        X, y = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "        y = to_categorical(y, num_classes=self.vocab)\n",
    "        loss = self.model.evaluate(X,y, verbose=0)\n",
    "        return np.exp(loss)\n",
    "        \n",
    "        \n",
    "    def prepare_data(self):\n",
    "        input_sequences = []\n",
    "        for line in self.data:\n",
    "            for i in range(1, len(line)):\n",
    "                n_gram_sequence = line[:i+1]\n",
    "                input_sequences.append(n_gram_sequence)\n",
    "        input_sequences = np.array(pad_sequences(input_sequences,maxlen = self.max_sequence_len, padding='pre'))\n",
    "        self.X, y = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "        self.y = to_categorical(y, num_classes=self.vocab)\n",
    "\n",
    "    def model_create(self):\n",
    "        input_len = self.max_sequence_len - 1\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(self.vocab, 128, input_length=input_len))\n",
    "        if self.hidden == \"Vanilla RNN\":\n",
    "            model.add(SimpleRNN(256))\n",
    "        elif self.hidden == \"LSTM\":\n",
    "            model.add(LSTM(256))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(self.vocab, activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "        print(model.summary())\n",
    "        model.fit(self.X, self.y, epochs=self.epochs, verbose=1, batch_size=self.batch_size)\n",
    "        return model\n",
    "        \n",
    "    def generate_text(self, n_sent):\n",
    "        in_text, result = [\"<s>\"], \"\"\n",
    "        sent_len=1\n",
    "        started = 0\n",
    "        while (n_sent>0):\n",
    "            encoded = np.array(self.tokenizer.texts_to_sequences([in_text])[0])\n",
    "            encoded = pad_sequences([encoded], maxlen= self.max_sequence_len-1, padding='pre')\n",
    "            probs = list(np.transpose(self.model.predict_proba(encoded, verbose=0))[:,0])\n",
    "            a=sum(probs)*1.01\n",
    "            probs=[i/a for i in probs]\n",
    "            out = list(np.random.multinomial(2,probs))\n",
    "            i = out.index(max(out))\n",
    "            out_word = self.indextoword[i]\n",
    "            if out_word==\"</s>\" or sent_len == self.max_sequence_len:\n",
    "                out_word = \".\"\n",
    "                in_text = [\"<s>\"]\n",
    "                n_sent-=1\n",
    "                sent_len=1\n",
    "                result = result + out_word\n",
    "                started=0\n",
    "            else:\n",
    "                in_text+=[out_word]\n",
    "                sent_len+=1\n",
    "                if not started:\n",
    "                    started = 1\n",
    "                    out_word = out_word.capitalize()\n",
    "                result = result + \" \" + out_word\n",
    "        return result.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 14, 128)           729088    \n",
      "_________________________________________________________________\n",
      "simple_rnn_6 (SimpleRNN)     (None, 256)               98560     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 5696)              1463872   \n",
      "=================================================================\n",
      "Total params: 2,291,520\n",
      "Trainable params: 2,291,520\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 147594 samples\n",
      "Epoch 1/20\n",
      "147594/147594 [==============================] - 8s 56us/sample - loss: 6.0810\n",
      "Epoch 2/20\n",
      "147594/147594 [==============================] - 7s 49us/sample - loss: 5.4172\n",
      "Epoch 3/20\n",
      "147594/147594 [==============================] - 7s 50us/sample - loss: 4.9148\n",
      "Epoch 4/20\n",
      "147594/147594 [==============================] - 7s 49us/sample - loss: 4.6276\n",
      "Epoch 5/20\n",
      "147594/147594 [==============================] - 8s 51us/sample - loss: 4.41250s - loss: 4.412\n",
      "Epoch 6/20\n",
      "147594/147594 [==============================] - 8s 51us/sample - loss: 4.2385\n",
      "Epoch 7/20\n",
      "147594/147594 [==============================] - 7s 49us/sample - loss: 4.0844\n",
      "Epoch 8/20\n",
      "147594/147594 [==============================] - 7s 49us/sample - loss: 3.94770s - loss: 3.94\n",
      "Epoch 9/20\n",
      "147594/147594 [==============================] - 7s 49us/sample - loss: 3.8256\n",
      "Epoch 10/20\n",
      "147594/147594 [==============================] - 8s 51us/sample - loss: 3.7116\n",
      "Epoch 11/20\n",
      "147594/147594 [==============================] - 7s 48us/sample - loss: 3.6095\n",
      "Epoch 12/20\n",
      "147594/147594 [==============================] - 7s 49us/sample - loss: 3.5093\n",
      "Epoch 13/20\n",
      "147594/147594 [==============================] - 7s 49us/sample - loss: 3.41860s - loss: \n",
      "Epoch 14/20\n",
      "147594/147594 [==============================] - 7s 49us/sample - loss: 3.3339\n",
      "Epoch 15/20\n",
      "147594/147594 [==============================] - 7s 49us/sample - loss: 3.2542\n",
      "Epoch 16/20\n",
      "147594/147594 [==============================] - 7s 48us/sample - loss: 3.1804\n",
      "Epoch 17/20\n",
      "147594/147594 [==============================] - 7s 49us/sample - loss: 3.11590s - lo\n",
      "Epoch 18/20\n",
      "147594/147594 [==============================] - 7s 48us/sample - loss: 3.0469\n",
      "Epoch 19/20\n",
      "147594/147594 [==============================] - 7s 49us/sample - loss: 2.9842\n",
      "Epoch 20/20\n",
      "147594/147594 [==============================] - 7s 49us/sample - loss: 2.9263\n"
     ]
    }
   ],
   "source": [
    "model_rnn = neural_model(data=data_train, tokenizer=tokenizer, word_index=word_index, index_word=index_word, max_sequence_len=15, hiddenlayer=\"Vanilla RNN\", epochs=20, batch_size=512)\n",
    "model_rnn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 14, 128)           729088    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 5696)              1463872   \n",
      "=================================================================\n",
      "Total params: 2,587,200\n",
      "Trainable params: 2,587,200\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 147594 samples\n",
      "Epoch 1/20\n",
      "147594/147594 [==============================] - 9s 64us/sample - loss: 6.0574\n",
      "Epoch 2/20\n",
      "147594/147594 [==============================] - 6s 44us/sample - loss: 5.4183\n",
      "Epoch 3/20\n",
      "147594/147594 [==============================] - 7s 45us/sample - loss: 5.0292\n",
      "Epoch 4/20\n",
      "147594/147594 [==============================] - 7s 45us/sample - loss: 4.8008\n",
      "Epoch 5/20\n",
      "147594/147594 [==============================] - 7s 44us/sample - loss: 4.6409\n",
      "Epoch 6/20\n",
      "147594/147594 [==============================] - 7s 45us/sample - loss: 4.5144\n",
      "Epoch 7/20\n",
      "147594/147594 [==============================] - 7s 45us/sample - loss: 4.4034\n",
      "Epoch 8/20\n",
      "147594/147594 [==============================] - 7s 45us/sample - loss: 4.3071\n",
      "Epoch 9/20\n",
      "147594/147594 [==============================] - 7s 46us/sample - loss: 4.2193\n",
      "Epoch 10/20\n",
      "147594/147594 [==============================] - 7s 47us/sample - loss: 4.1403\n",
      "Epoch 11/20\n",
      "147594/147594 [==============================] - 8s 52us/sample - loss: 4.0644\n",
      "Epoch 12/20\n",
      "147594/147594 [==============================] - 7s 45us/sample - loss: 3.9963\n",
      "Epoch 13/20\n",
      "147594/147594 [==============================] - 7s 49us/sample - loss: 3.9310\n",
      "Epoch 14/20\n",
      "147594/147594 [==============================] - 7s 49us/sample - loss: 3.8695\n",
      "Epoch 15/20\n",
      "147594/147594 [==============================] - 7s 46us/sample - loss: 3.8109\n",
      "Epoch 16/20\n",
      "147594/147594 [==============================] - 7s 45us/sample - loss: 3.7578\n",
      "Epoch 17/20\n",
      "147594/147594 [==============================] - 7s 45us/sample - loss: 3.7047\n",
      "Epoch 18/20\n",
      "147594/147594 [==============================] - 7s 45us/sample - loss: 3.6560\n",
      "Epoch 19/20\n",
      "147594/147594 [==============================] - 7s 45us/sample - loss: 3.6078\n",
      "Epoch 20/20\n",
      "147594/147594 [==============================] - 7s 45us/sample - loss: 3.5634\n"
     ]
    }
   ],
   "source": [
    "model_lstm = neural_model(data=data_train, tokenizer=tokenizer, word_index=word_index, index_word=index_word, max_sequence_len=15, hiddenlayer=\"LSTM\", epochs=20, batch_size=512)\n",
    "model_lstm.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Text using the neural models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla RNN model\n",
      "\n",
      "\n",
      "Then i will certainly they will get away in the history of the gsa. Its got to be very very well. I dont know. I mean the veterans and the one thing. We have a situation we have to change.\n",
      "\n",
      "\n",
      "LSTM model\n",
      "\n",
      "\n",
      "I dont know it. And we. And i said \"let me tell you. I was in last. And i have a great honor.\n"
     ]
    }
   ],
   "source": [
    "print(\"Vanilla RNN model\\n\\n\")\n",
    "print(model_rnn.generate_text(5))\n",
    "print(\"\\n\\nLSTM model\\n\\n\")\n",
    "print(model_lstm.generate_text(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data generated by these models are pretty readable. Most of the sentences are almost grammatically correct too. They actually seem like sentences from daily conversations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity of the neural models\n",
    "\n",
    "For this, I am using the in-built function evaluate to calculate the cross-entropy of the test data. Then, perplexity is just exp(cross entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla RNN model:\n",
      "Perplexity: 117.25450446445656\n",
      "\n",
      "LSTM model:\n",
      "Perplexity: 104.8216095336064\n"
     ]
    }
   ],
   "source": [
    "print(\"Vanilla RNN model:\")\n",
    "print(\"Perplexity:\", model_rnn.evaluate(data_test))\n",
    "print(\"\\nLSTM model:\")\n",
    "print(\"Perplexity:\", model_lstm.evaluate(data_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity of the LSTM is slightly better than that of the Vanilla RNN model. This is because LSTM improve on the Vanilla RNN model by storing some memory. Hence, they perform better as they are expected to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Neural approach works better because of the power of RNNs. Also, there is more context covered in the RNNs than the ngram models. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
